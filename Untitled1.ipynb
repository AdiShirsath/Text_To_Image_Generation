{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851689b5-5695-4a1d-890d-30bda456c8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Metal\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbf2aa1-ed0e-41f4-94e5-f51bdcc2babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "# torch.backends.mps.enable_flash_sdp(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc85dc1-889f-41dc-89fa-84071367c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intro_dl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from scripts import eng_processor\n",
    "from scripts.text_preprocessing import TextImageDataset\n",
    "from scripts.train_model import BertEncoder, Generator, Discriminator\n",
    "from scripts.inference import generate_image_from_text\n",
    "from scripts.utils import combine_dataset, Evaluator\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7da7f8b-322d-4e7c-b64c-98d827943ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90483</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A black cat looking out the window at a black ...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90484</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>Black cat sitting on window ledge looking outs...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90485</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A black cat looks out the window as a crow out...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90486</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A cat by a window with a small bird outside.</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90487</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A cat watches a bird through a window.</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  \\\n",
       "90483  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90484  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90485  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90486  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90487  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "\n",
       "                                                 caption  source  \n",
       "90483  A black cat looking out the window at a black ...  MSCOCO  \n",
       "90484  Black cat sitting on window ledge looking outs...  MSCOCO  \n",
       "90485  A black cat looks out the window as a crow out...  MSCOCO  \n",
       "90486       A cat by a window with a small bird outside.  MSCOCO  \n",
       "90487             A cat watches a bird through a window.  MSCOCO  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.read_csv(\"datasets/combined_dataset.csv\")\n",
    "combined_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5831f18-8ec9-4621-8394-fa0119f89793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/8k8d_tn97jdgsjynhl5q7yr80000gn/T/ipykernel_86097/1002816691.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  combined_df.caption[i] = eng_processor.main(combined_df.caption[i])\n"
     ]
    }
   ],
   "source": [
    "# combined_df= combined_df.where(combined_df[\"source\"]==\"Flickr\")\n",
    "\n",
    "# combined_df = combined_df.dropna()\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).head(10000)\n",
    "combined_df = combined_df.reset_index(drop=True)  # Reset index to sequential integers\n",
    "\n",
    "for i in range(len(combined_df)):\n",
    "    combined_df.caption[i] = eng_processor.main(combined_df.caption[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cd6c30-878b-4d76-9100-525a61a29b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_captions = defaultdict(list)\n",
    "\n",
    "for _, row in combined_df.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    image_to_captions[img_path].append(row['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c610ae10-20d4-4904-9d17-52429893680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(image_to_captions.keys())\n",
    "train_ids, test_ids = train_test_split(image_paths, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33567146-e532-4ba7-baf5-2add698823cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8032 (all captions included)\n",
      "Dataset size: 1967 (all captions included)\n"
     ]
    }
   ],
   "source": [
    "# image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Lambda(lambda x: (x * 2) - 1)  # Scale from [0,1] to [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = TextImageDataset(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_paths=train_ids,  \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = TextImageDataset(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_paths=test_ids,  \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85c3358-8e62-45cb-aba7-c0cec12e9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9d81d7-17aa-44a0-adce-fa8ca4234b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_text_encoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3c9dd4-876b-4f3b-a083-e4be0ee745ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:03<00:00,  2.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.33.1\",\n",
       "  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained pipeline\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",  # or any other model\n",
    "    torch_dtype=torch.float16,         # Use float16 only if GPU/MLCompute is supported\n",
    ")\n",
    "\n",
    "pipeline.to(\"mps\")  # Use M1/M2 GPU via Metal backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdc6c91-0db6-4df9-8155-0ed79779ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_dim=768, out_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "projection = Projection().to(device)\n",
    "\n",
    "# === 4. Stable Diffusion Decoder ===\n",
    "vae = pipeline.vae.to(device)\n",
    "unet = pipeline.unet.to(device)\n",
    "scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd4fd84-9037-43dc-96d5-ffe900bdf1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everything except projection\n",
    "for param in vae.parameters(): param.requires_grad = False\n",
    "for param in unet.parameters(): param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(projection.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc54e2b-9556-4318-8806-32cc18a38c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                      | 1/4016 [00:01<1:23:43,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0/4016] [Loss: 0.3232]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▌                     | 101/4016 [00:40<25:30,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 100/4016] [Loss: 0.3823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|█                     | 201/4016 [01:19<25:03,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 200/4016] [Loss: 0.3982]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|█▋                    | 301/4016 [02:01<26:38,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 300/4016] [Loss: 0.3384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  10%|██▏                   | 401/4016 [02:40<23:27,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 400/4016] [Loss: 0.3679]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  12%|██▋                   | 501/4016 [03:20<24:02,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 500/4016] [Loss: 0.4478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  15%|███▎                  | 601/4016 [03:59<21:57,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 600/4016] [Loss: 0.3013]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  17%|███▊                  | 701/4016 [04:38<22:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 700/4016] [Loss: 0.4221]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  20%|████▍                 | 801/4016 [05:18<20:42,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 800/4016] [Loss: 0.4031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  22%|████▉                 | 901/4016 [05:57<19:57,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 900/4016] [Loss: 0.5649]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  25%|█████▏               | 1001/4016 [06:36<20:12,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1000/4016] [Loss: 0.2632]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  27%|█████▊               | 1101/4016 [07:15<19:02,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1100/4016] [Loss: 0.3904]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  30%|██████▎              | 1201/4016 [07:55<17:54,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1200/4016] [Loss: 0.3252]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  32%|██████▊              | 1301/4016 [08:34<17:43,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1300/4016] [Loss: 0.2568]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  35%|███████▎             | 1401/4016 [09:13<17:29,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1400/4016] [Loss: 0.3064]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  37%|███████▊             | 1501/4016 [09:53<16:23,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1500/4016] [Loss: 0.3442]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  40%|████████▎            | 1601/4016 [10:32<16:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1600/4016] [Loss: 0.3782]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  42%|████████▉            | 1701/4016 [11:11<14:51,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1700/4016] [Loss: 0.3552]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  45%|█████████▍           | 1801/4016 [11:50<14:26,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1800/4016] [Loss: 0.3550]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  47%|█████████▉           | 1901/4016 [12:30<13:34,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1900/4016] [Loss: 0.5303]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  50%|██████████▍          | 2001/4016 [13:09<13:15,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2000/4016] [Loss: 0.3474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  52%|██████████▉          | 2101/4016 [13:48<12:22,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2100/4016] [Loss: 0.3489]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  55%|███████████▌         | 2201/4016 [14:28<11:58,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2200/4016] [Loss: 0.2708]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  57%|████████████         | 2301/4016 [15:07<11:02,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2300/4016] [Loss: 0.4895]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  60%|████████████▌        | 2401/4016 [15:46<10:32,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2400/4016] [Loss: 0.2231]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  62%|█████████████        | 2501/4016 [16:25<09:52,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2500/4016] [Loss: 0.4858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  65%|█████████████▌       | 2601/4016 [17:04<09:15,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2600/4016] [Loss: 0.3733]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  67%|██████████████       | 2701/4016 [17:44<08:44,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2700/4016] [Loss: 0.1864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  70%|██████████████▋      | 2801/4016 [18:23<07:53,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2800/4016] [Loss: 0.4731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  72%|███████████████▏     | 2901/4016 [19:02<07:17,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2900/4016] [Loss: 0.4316]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  75%|███████████████▋     | 3001/4016 [19:42<06:37,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3000/4016] [Loss: 0.3027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  77%|████████████████▏    | 3101/4016 [20:21<05:57,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3100/4016] [Loss: 0.3328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  80%|████████████████▋    | 3201/4016 [21:00<05:32,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3200/4016] [Loss: 0.3018]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  82%|█████████████████▎   | 3301/4016 [21:39<04:38,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3300/4016] [Loss: 0.3267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  85%|█████████████████▊   | 3401/4016 [22:19<04:05,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3400/4016] [Loss: 0.2256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  87%|██████████████████▎  | 3501/4016 [22:58<03:21,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3500/4016] [Loss: 0.4500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  90%|██████████████████▊  | 3601/4016 [23:37<02:43,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3600/4016] [Loss: 0.3853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  92%|███████████████████▎ | 3701/4016 [24:16<02:02,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3700/4016] [Loss: 0.2593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  95%|███████████████████▉ | 3801/4016 [25:05<02:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3800/4016] [Loss: 0.3101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  97%|████████████████████▍| 3901/4016 [26:13<01:16,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3900/4016] [Loss: 0.3748]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████▉| 4001/4016 [27:18<00:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 4000/4016] [Loss: 0.3196]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|█████████████████████| 4016/4016 [27:28<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   0%|                        | 1/4016 [00:00<43:03,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0/4016] [Loss: 0.4053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   3%|▌                     | 101/4016 [01:06<42:21,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 100/4016] [Loss: 0.2140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   5%|█                     | 201/4016 [02:11<42:17,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 200/4016] [Loss: 0.3354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   7%|█▋                    | 301/4016 [03:17<40:12,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 300/4016] [Loss: 0.1713]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  10%|██▏                   | 401/4016 [22:08<42:07,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 400/4016] [Loss: 0.3389]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  12%|██▋                   | 501/4016 [23:16<41:09,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 500/4016] [Loss: 0.3582]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  15%|███▎                  | 601/4016 [24:29<44:26,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 600/4016] [Loss: 0.2600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  17%|███▊                  | 701/4016 [25:43<37:33,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 700/4016] [Loss: 0.3657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  20%|████▍                 | 801/4016 [26:51<26:47,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 800/4016] [Loss: 0.5713]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  22%|████▉                 | 901/4016 [27:31<20:32,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 900/4016] [Loss: 0.3525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  25%|█████▏               | 1001/4016 [28:11<19:33,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1000/4016] [Loss: 0.2876]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  27%|█████▊               | 1101/4016 [28:53<21:27,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1100/4016] [Loss: 0.1978]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  30%|██████▎              | 1201/4016 [29:37<22:10,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1200/4016] [Loss: 0.5640]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  32%|██████▊              | 1301/4016 [30:19<17:22,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1300/4016] [Loss: 0.2062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  35%|███████▎             | 1401/4016 [30:58<17:07,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1400/4016] [Loss: 0.2712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  37%|███████▊             | 1501/4016 [31:39<17:27,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1500/4016] [Loss: 0.3359]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  40%|████████▎            | 1601/4016 [32:19<16:10,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1600/4016] [Loss: 0.3535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  42%|████████▉            | 1701/4016 [33:00<16:07,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1700/4016] [Loss: 0.2106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  45%|█████████▍           | 1801/4016 [33:40<14:16,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1800/4016] [Loss: 0.2549]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  47%|█████████▉           | 1901/4016 [34:22<15:41,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1900/4016] [Loss: 0.3665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  50%|██████████▍          | 2001/4016 [35:02<13:14,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2000/4016] [Loss: 0.3787]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  52%|██████████▉          | 2101/4016 [35:41<12:27,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2100/4016] [Loss: 0.1720]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  55%|███████████▌         | 2201/4016 [36:24<12:05,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2200/4016] [Loss: 0.6113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  57%|████████████         | 2301/4016 [37:04<10:57,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2300/4016] [Loss: 0.3308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  60%|████████████▌        | 2401/4016 [37:43<10:29,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2400/4016] [Loss: 0.4697]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  62%|█████████████        | 2501/4016 [38:23<09:58,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2500/4016] [Loss: 0.2725]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  65%|█████████████▌       | 2601/4016 [39:03<09:22,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2600/4016] [Loss: 0.2915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  67%|██████████████       | 2701/4016 [39:42<08:43,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2700/4016] [Loss: 0.3047]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  70%|██████████████▋      | 2801/4016 [40:22<08:09,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2800/4016] [Loss: 0.3347]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  72%|███████████████▏     | 2901/4016 [41:01<07:17,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2900/4016] [Loss: 0.2917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  73%|███████████████▏     | 2914/4016 [41:07<07:23,  2.48it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    unet.train()\n",
    "    clip_text_encoder.eval()\n",
    "    vae.eval()\n",
    "    projection.train()\n",
    "\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        texts = batch[\"caption_text\"]\n",
    "        \n",
    "        # Match input dtype with model\n",
    "        dtype = next(vae.parameters()).dtype\n",
    "        images = images.to(dtype=dtype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Text encoding\n",
    "            inputs = clip_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            text_feats = clip_text_encoder(**inputs).last_hidden_state  # (B, seq_len, 768)\n",
    "\n",
    "\n",
    "            # get latent for vae\n",
    "            latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "\n",
    "        pooled_feats = text_feats.mean(dim=1)  # (B, 768)\n",
    "        projected_feats = projection(pooled_feats)\n",
    "\n",
    "        # add noise to it\n",
    "        with torch.no_grad():\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        projected_feats = projected_feats.to(dtype=noisy_latents.dtype).unsqueeze(1)  # (B, 1, D)\n",
    "\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=projected_feats).sample\n",
    "\n",
    "        loss = loss_fn(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Batch {i}/{len(train_loader)}] [Loss: {loss.item():.4f}]\")\n",
    "\n",
    "        # Clear memory\n",
    "        del images, texts, inputs, text_feats, pooled_feats, projected_feats\n",
    "        del latents, noise, timesteps, noisy_latents, noise_pred\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch} finished\")\n",
    "\n",
    "\n",
    "# Save model after training\n",
    "torch.save({\n",
    "    'unet': unet.state_dict(),\n",
    "    'clip_text_encoder': clip_text_encoder.state_dict(),\n",
    "    'projection': projection.state_dict()\n",
    "}, \"final_model_checkpoint.pt\")\n",
    "\n",
    "print(\"Training completed ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd37bce-8ea9-4ba1-9912-d2c7a5680e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98b29d-33a3-4567-af78-91a1b1c54a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9650d2-7751-4275-9a3d-4f1b98ff8ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421783c-53df-40bf-8298-e637d1246c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033d401-91a5-45b3-aee0-e4b01ab6a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_dl",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
