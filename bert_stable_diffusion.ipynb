{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35a2143-beb1-4cb5-9639-b1cf8e19b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intro_dl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/intro_dl/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from scripts import eng_processor\n",
    "from scripts.text_preprocessing import TextImageDataset\n",
    "from scripts.train_model import BertEncoder, Generator, Discriminator\n",
    "from scripts.inference import generate_image_from_text\n",
    "from scripts.utils import combine_dataset, Evaluator\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c16792b-c0b5-4cc8-9a22-cf68c1803288",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "# torch.backends.mps.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b627e3-dff3-4385-95bd-37999ba62c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Metal\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c087fd96-2452-4c21-849b-8af203d4959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90483</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A black cat looking out the window at a black ...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90484</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>Black cat sitting on window ledge looking outs...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90485</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A black cat looks out the window as a crow out...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90486</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A cat by a window with a small bird outside.</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90487</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000370808.jpg</td>\n",
       "      <td>A cat watches a bird through a window.</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  \\\n",
       "90483  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90484  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90485  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90486  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "90487  datasets/coco_dataset/train2017/000000370808.jpg   \n",
       "\n",
       "                                                 caption  source  \n",
       "90483  A black cat looking out the window at a black ...  MSCOCO  \n",
       "90484  Black cat sitting on window ledge looking outs...  MSCOCO  \n",
       "90485  A black cat looks out the window as a crow out...  MSCOCO  \n",
       "90486       A cat by a window with a small bird outside.  MSCOCO  \n",
       "90487             A cat watches a bird through a window.  MSCOCO  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.read_csv(\"datasets/combined_dataset.csv\")\n",
    "combined_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26a4c49-3260-43c7-89b1-527cc3d77a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/8k8d_tn97jdgsjynhl5q7yr80000gn/T/ipykernel_2563/1002816691.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  combined_df.caption[i] = eng_processor.main(combined_df.caption[i])\n"
     ]
    }
   ],
   "source": [
    "# combined_df= combined_df.where(combined_df[\"source\"]==\"Flickr\")\n",
    "\n",
    "# combined_df = combined_df.dropna()\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).head(10000)\n",
    "combined_df = combined_df.reset_index(drop=True)  # Reset index to sequential integers\n",
    "\n",
    "for i in range(len(combined_df)):\n",
    "    combined_df.caption[i] = eng_processor.main(combined_df.caption[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10a51a7-05c6-412b-adee-fe0f4d1c1c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datasets/Flickr8k_Dataset/1957371533_62bc720ba...</td>\n",
       "      <td>a dog shaking water off</td>\n",
       "      <td>Flickr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datasets/Flickr8k_Dataset/3411022255_210eefc37...</td>\n",
       "      <td>four people and a child walking in the street</td>\n",
       "      <td>Flickr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datasets/Flickr8k_Dataset/2294516804_11e255807...</td>\n",
       "      <td>a baby sits by his red and green toy pulling a...</td>\n",
       "      <td>Flickr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000331372.jpg</td>\n",
       "      <td>an image from the outside of a window with flo...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000527711.jpg</td>\n",
       "      <td>a close up of a person talking on a cell phone</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000189461.jpg</td>\n",
       "      <td>an assortment of donuts in front of a coffee cup</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000344314.jpg</td>\n",
       "      <td>the two zebras are standing behind the warning...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>datasets/Flickr8k_Dataset/505929313_7668f021ab...</td>\n",
       "      <td>a black dog standing in shallow water with a p...</td>\n",
       "      <td>Flickr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>datasets/coco_dataset/train2017/000000119384.jpg</td>\n",
       "      <td>a close up of a car parked on top of a small f...</td>\n",
       "      <td>MSCOCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>datasets/Flickr8k_Dataset/3255732353_fbc487aef...</td>\n",
       "      <td>a child in pink throwing a snowball at a child...</td>\n",
       "      <td>Flickr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  \\\n",
       "0     datasets/Flickr8k_Dataset/1957371533_62bc720ba...   \n",
       "1     datasets/Flickr8k_Dataset/3411022255_210eefc37...   \n",
       "2     datasets/Flickr8k_Dataset/2294516804_11e255807...   \n",
       "3      datasets/coco_dataset/train2017/000000331372.jpg   \n",
       "4      datasets/coco_dataset/train2017/000000527711.jpg   \n",
       "...                                                 ...   \n",
       "9995   datasets/coco_dataset/train2017/000000189461.jpg   \n",
       "9996   datasets/coco_dataset/train2017/000000344314.jpg   \n",
       "9997  datasets/Flickr8k_Dataset/505929313_7668f021ab...   \n",
       "9998   datasets/coco_dataset/train2017/000000119384.jpg   \n",
       "9999  datasets/Flickr8k_Dataset/3255732353_fbc487aef...   \n",
       "\n",
       "                                                caption  source  \n",
       "0                               a dog shaking water off  Flickr  \n",
       "1         four people and a child walking in the street  Flickr  \n",
       "2     a baby sits by his red and green toy pulling a...  Flickr  \n",
       "3     an image from the outside of a window with flo...  MSCOCO  \n",
       "4        a close up of a person talking on a cell phone  MSCOCO  \n",
       "...                                                 ...     ...  \n",
       "9995   an assortment of donuts in front of a coffee cup  MSCOCO  \n",
       "9996  the two zebras are standing behind the warning...  MSCOCO  \n",
       "9997  a black dog standing in shallow water with a p...  Flickr  \n",
       "9998  a close up of a car parked on top of a small f...  MSCOCO  \n",
       "9999  a child in pink throwing a snowball at a child...  Flickr  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d4e99ca-1ed2-4fde-9bcb-c53e9df10465",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_captions = defaultdict(list)\n",
    "\n",
    "for _, row in combined_df.iterrows():\n",
    "    img_path = row['image_path']\n",
    "    image_to_captions[img_path].append(row['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e924971e-b43d-4d0f-92f6-b53911d38bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(image_to_captions.keys())\n",
    "train_ids, test_ids = train_test_split(image_paths, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1310b985-5e94-4a1f-bda9-dae571b4d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Collect images and tokenize captions\n",
    "    images = torch.stack([example['image'] for example in batch])  # Stack images\n",
    "    input_ids = torch.stack([example['input_ids'] for example in batch])  # Stack input_ids\n",
    "    attention_mask = torch.stack([example['attention_mask'] for example in batch])  # Stack attention_mask\n",
    "\n",
    "    return {\n",
    "        'pixel_values': images,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfdc5bcd-7995-417b-8efd-e2d4c9b4d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7988 (all captions included)\n",
      "Dataset size: 2011 (all captions included)\n"
     ]
    }
   ],
   "source": [
    "# image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = TextImageDataset(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_paths=train_ids,  \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = TextImageDataset(\n",
    "    image_to_captions=image_to_captions,\n",
    "    image_paths=test_ids,  \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032d2d5-43f1-4717-8ecd-5d1bd06bdeca",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c181ce48-aac8-47e8-bac0-6d99a8ca1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q diffusers accelerate transformers safetensors peft bitsandbytes datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8f2cea-94cc-4351-8158-2cb5c0dac54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/peft.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23522a0-d971-4009-902b-8a01f3efbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from peft import LoraModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e20dc6e5-bd09-4fb8-8770-7e67692fbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load the BERT model to get embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eab7f45-c66d-4c11-b6b4-fa29e61286fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|███████████████| 7/7 [00:01<00:00,  5.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.unet.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5028d769-5dd9-49c2-bc6e-7fc1f7deaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = pipe.vae\n",
    "unet = pipe.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65abd23-745f-4a63-b23e-d736c55a373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToLatent(nn.Module):\n",
    "    def __init__(self, text_dim=768, latent_dim=4*64*64):  # SD latent size: (4, 64, 64)\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(text_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).view(-1, 4, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b97d525b-11af-4e08-9cfe-dc1705eda0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = TextToLatent().to(device)\n",
    "optimizer = torch.optim.Adam(mapper.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863d358-d418-4dcc-8019-6984187e73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|▏                 | 30/3994 [01:31<3:22:57,  3.07s/it, loss=0.749]"
     ]
    }
   ],
   "source": [
    "vae = vae.to(\"cpu\").float()   # VAE stays on CPU\n",
    "bert = bert.to(device).float()\n",
    "mapper = mapper.to(device).float()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    mapper.train()\n",
    "    bert.eval()\n",
    "    vae.eval()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch[\"attention_mask\"].to(device)\n",
    "        images = batch[\"image\"].float().to(\"cpu\")  # images to CPU for VAE\n",
    "\n",
    "        # 1. Get text embeddings\n",
    "        with torch.no_grad():\n",
    "            text_embeds = bert(input_ids=input_ids, attention_mask=attn_mask).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # 2. Get latent encodings from VAE on CPU\n",
    "        with torch.no_grad():\n",
    "            latents_gt = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "            latents_gt = latents_gt.to(device)  # move latents to MPS for training\n",
    "\n",
    "        # 3. Predict and compute loss\n",
    "        latents_pred = mapper(text_embeds)\n",
    "        loss = loss_fn(latents_pred, latents_gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": total_loss / (pbar.n + 1)})\n",
    "\n",
    "        # Free memory\n",
    "        del input_ids, attn_mask, images, text_embeds, latents_gt, latents_pred\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Save trained mapper\n",
    "torch.save(mapper.state_dict(), \"text_to_latent_mapper.pth\")\n",
    "print(\"✅ Done training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74125c4e-ae31-4765-9420-6ef27eb6dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_dl",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
